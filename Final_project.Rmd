---
title: "Final Project"
author: "Peter George"
date: "October 18, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(knitr)
library(dplyr)
library(readxl)
library(janitor)
```

```{r, message=FALSE, cache=TRUE}
##unzip("Shot2018-302.zip")

x <- read_excel("ThePlayers.xlsx") %>%
  clean_names()

x %>%
  select(player_first_name, player_last_name, round, hole, strokes_gained_baseline, player_number) %>%
  group_by(player_last_name, round) %>%
  filter(player_number %in% c(29221,26331,25632,48081,28089,25686,33141,37189,30911,29926,36689,33448,34563,24502,24138,8793,27349,29461,21961,26499)) %>%
  mutate(total_strokes_gained = sum(strokes_gained_baseline)) 
  

write_rds(x, "finalproject.rds")
```

 ## I eventually want the shiny app to run with the bins being the rounds and have the strokes gained per round be bars. In order to do that I have to make the graphs larger or I have to maybe narrow down the playrs to just the top 30 in the tournament or maybe even 20.

```{r}
# I plan on using the PGA Tour Shotlink Data in order to test how well strokes gained anticipates how well a player will do in a tournament. Even though this is what I am thinking right now, after looking at the data I might find that I am better served looking at something else in the data set and then I will observe that. Also, there is so much data that when I try to read it into R Studio, my computer basically crashes... so that is something I'll have to deal with going forward.

##xx <- read_excel("Part of 2018 Data.xlsx")
```
